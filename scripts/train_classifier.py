from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from datetime import datetime, timedelta
from time import sleep
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from agents.networks.classifier import PriceMovementClassifier, AttentionGRUClassifier
from config.model import PMCModelConfig
from data.preprocess.ohlcv_data import preprocess_candles, preprocess_sequence_input, create_labels, normalize_features
from data.upbit_api import UpbitAPI

# Dataset
class OHLCVDataset(Dataset):
    def __init__(self, df: pd.DataFrame, window: int):
        feature_cols = [
            "open", "high", "low", "close", "volume",
            "ema5", "ema10", "ema20", "rsi14",
            "macd", "macd_signal", "macd_hist",
            "bb_ema_upper", "bb_ema_lower",
            "vol_ma5", "vol_ma20"
        ]
        self.features = preprocess_sequence_input(df, window, feature_cols=feature_cols)[:-1]
        self.labels = create_labels(df, window)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return (
            torch.tensor(self.features[idx], dtype=torch.float32),
            torch.tensor(self.labels[idx], dtype=torch.long)
        )


def train(
    model,
    dataloader, 
    val_loader,
    epochs: int = 10, 
    lr: float = 1e-4, 
    save_path: str = "models/price_classifier.pt",
    log_freq: int = 100,
    show_plot: bool = False,
    plot_path: str = "etc/loss_acc_plot.png"
) -> None:
    """
    ì‹œê³„ì—´ ê°€ê²© ì¶”ì„¸ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜.

    Parameters:
        model (PriceDirectionClassifier): ìƒìŠ¹/í•˜ë½ ì¶”ì„¸ ë¶„ë¥˜ ëª¨ë¸
        dataloader (torch.utils.data.DataLoader): í•™ìŠµ ë°ì´í„° ë¡œë” 
        epochs (int): í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ (default: 10)
        lr (float): í•™ìŠµë¥  (default: 1e-4)
        save_path (str): í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ (default: models/price_classifier.pt)
        log_freq (int): í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥ ë¹ˆë„ (default: 100 stepë§ˆë‹¤)
        show_plot (bool): Trueì´ë©´ í•™ìŠµ ì •í™•ë„/ì†ì‹¤ ê·¸ë˜í”„ë¥¼ ì¶œë ¥ (default: False)

    Returns:
        None
    """
    # GPU ì‚¬ìš© (ë¶ˆê°€ëŠ¥í•˜ë©´ CPU ì‚¬ìš©)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Using device: {device}")

    # ëª¨ë¸ì„ GPU (í˜¹ì€ CPU)ë¡œ ì˜®ê¹€
    model = model.to(device)
    # ì—í­ ë° í•™ìŠµë¥  ì„¤ì •
    #epochs = PMCModelConfig.epochs
    lr = PMCModelConfig.lr
    # ì†ì‹¤í•¨ìˆ˜ ì„¤ì • -> ê°€ì¤‘ì¹˜ ì ìš©í•¨
    #loss_fn_class = getattr(torch.nn, PMCModelConfig.loss_fn)
    #loss_fn = loss_fn_class()
    #torch.nn.CrossEntropyLoss
    all_labels = torch.cat([y for _, y in dataloader], dim=0).cpu().numpy()
    weight_tensor = torch.tensor([0.8, 1.5, 1.5], dtype=torch.float32).to(device)
    loss_fn = torch.nn.CrossEntropyLoss(weight=weight_tensor)
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optim_class = getattr(torch.optim, PMCModelConfig.optim)
    optimizer = optim_class(model.parameters(), lr=lr)
    #ìµœì ëª¨ë¸ì„ ì €ì¥í•˜ê¸°ìœ„í•œ ë³€ìˆ˜ ì„¤ì •ì •
    best_val_acc = 0.0
    patience = 100
    no_improve_epochs = 0
    # ReduceLROnPlateau ì„¤ì •
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=10
    )

    # ì§€í‘œ ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸ ë³€ìˆ˜
    if show_plot:
        loss_list = {"train": [], "val": []}    # ì¶”í›„ ê²€ì¦ìš© ë°ì´í„°ë„ ë„£ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ë”°ë¡œ ì¶”ê°€í•´ë‘ 
        accuracy_list = {"train": [], "val": []}

    
    # ë°ì´í„°ì…‹ì„ epochsë§Œí¼ ë°˜ë³µ
    for epoch in range(epochs):
        # ëª¨ë¸ í•™ìŠµ ì‹œì‘
        model.train()

        total_loss = 0
        correct = 0
        total = 0
        for _, data in enumerate(dataloader):
            # dataì—ì„œ input ê°’(inputs)ê³¼ ì •ë‹µ(labels) ë°›ì•„ì˜¤ê¸°
            inputs, labels = data
            # inputê³¼ label ë°ì´í„°ë¥¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¹€
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            # gradient ê°’ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
            optimizer.zero_grad()

            # ìˆœì „íŒŒ
            preds = model(inputs)
            loss = loss_fn(preds, labels)
            # ì—­ì „íŒŒ
            loss.backward()
            # ì˜µí‹°ë§ˆì´ì €ë¥¼ í†µí•´ ìµœì í™”
            optimizer.step()

            # ì´ ì†ì‹¤ê°’ ì €ì¥
            total_loss += loss.item()
            # ë§ì¶˜ ë°ì´í„° ê°œìˆ˜ ì €ì¥
            correct += (preds.argmax(dim=1) == labels).sum().item()
            # ì „ì²´ ë°ì´í„° ê°œìˆ˜ ì €ì¥
            total += labels.size(0)

        # --- ê²€ì¦ ---
        model.eval()
        val_loss, val_correct, val_total = 0, 0, 0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                preds = model(xb)
                val_loss += loss_fn(preds, yb).item()
                val_correct += (preds.argmax(1) == yb).sum().item()
                val_total += yb.size(0)
        
        avg_val_loss = val_loss / len(val_loader)
        avg_val_acc = val_correct / val_total

        scheduler.step(val_loss)

        # ì¶œë ¥ ë° ê¸°ë¡
        if (epoch + 1) % log_freq == 0:
            print(f"[Epoch {epoch + 1:3d}] Train Loss: {total_loss:2.4f}, Accuracy: {correct / total:.8f} | Val Loss: {val_loss:.4f}, Acc: {avg_val_acc:.4f}")

        if show_plot:
            avg_loss = total_loss / len(dataloader)
            avg_acc = correct / total
            loss_list["train"].append(avg_loss)
            accuracy_list["train"].append(avg_acc)
            loss_list["val"].append(avg_val_loss)
            accuracy_list["val"].append(avg_val_acc)
    # ëª¨ë¸ ì €ì¥
        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc
            no_improve_epochs = 0
            torch.save(model.state_dict(), save_path)
        else:
            no_improve_epochs += 1
        if no_improve_epochs >= patience:
            print(f"Early stopping triggered at epoch {epoch}")
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
            break

    # ê·¸ë˜í”„ ì¶œë ¥
    if show_plot:
        plt.figure(figsize=(8,3))
        # í›ˆë ¨ ì†ì‹¤ ê·¸ë˜í”„
        plt.subplot(121)
        plt.plot(loss_list["train"])
        plt.plot(loss_list["val"])
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend(['train', 'val'], loc='upper right')

        # í›ˆë ¨ ì •í™•ë„ ê·¸ë˜í”„
        plt.subplot(122)
        plt.plot(accuracy_list["train"])
        plt.plot(accuracy_list["val"])
        plt.title('Model Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend(['train', 'val'], loc='lower right')
        plt.tight_layout()
        plt.savefig(plot_path)
        print(f"í•™ìŠµ ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {plot_path}")
        plt.show()
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        all_preds = []
        all_labels = []
        model.eval()
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                preds = model(xb)
                pred_labels = preds.argmax(1).cpu().numpy()
                all_preds.extend(pred_labels)
                all_labels.extend(yb.numpy())

        cm = confusion_matrix(all_labels, all_preds)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Change", "Up", "Down"])
        fig, ax = plt.subplots(figsize=(5, 4))
        disp.plot(cmap="Blues", values_format="d", ax=ax)
        plt.title("Confusion Matrix")
        plt.tight_layout()
        plt.savefig("etc/confusion_matrix.png")
        print("í˜¼ë™ í–‰ë ¬ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: etc/confusion_matrix.png")
        plt.show()

        all_preds = []
        all_labels = []

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                preds = torch.argmax(outputs, dim=1)

                all_preds.extend(preds.cpu().tolist())
                all_labels.extend(labels.cpu().tolist())

        # ğŸ”½ F1-score, precision, recall ì¶œë ¥
        print(classification_report(
            all_labels, all_preds,
            target_names=["No Change", "Up", "Down"]
        ))

if __name__ == "__main__":
    # ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    api = UpbitAPI()
    dt = datetime(2025, 2, 28, 0, 0, 0)
    dfs = []  # ì—¬ëŸ¬ ê°œì˜ DataFrame ì €ì¥ ë¦¬ìŠ¤íŠ¸

    for i in range(100):
        raw_data = api.get_candles("KRW-XRP", unit="minute", interval=1, count=200, to=dt)
        #print(raw_data)
        df = preprocess_candles(raw_data)
        print(f"Data count: {i * 200}, Date: {dt}")
        dfs.append(df)

        # ê°€ì¥ ê³¼ê±° ì‹œì  ê¸°ì¤€ìœ¼ë¡œ to ê°’ì„ ì—…ë°ì´íŠ¸ (UpbitëŠ” ê³¼ê±°ë¡œ ì¡°íšŒë¨)
        earliest_time = df.iloc[-1]["datetime"]
        # ë‹¤ìŒ ìš”ì²­ ì‹œì 
        dt = earliest_time + timedelta(minutes=1) - timedelta(hours=5, minutes=40)  
        sleep(0.1)

    # ì „ì²´ ë°ì´í„°í”„ë ˆì„ ì—°ê²°
    full_df = pd.concat(dfs, ignore_index=True).sort_values("datetime").reset_index(drop=True)
    full_df = full_df.dropna().reset_index(drop=True) #ì´ê±° í•´ì¤€ê±° ì•„ë‹Œê°€?

    # âœ… feature ì»¬ëŸ¼ ì •ì˜
    feature_cols = [
        "open", "high", "low", "close", "volume",
        "ema5", "ema10", "ema20", "rsi14",
        "macd", "macd_signal", "macd_hist",
        "bb_ema_upper", "bb_ema_lower",
        "vol_ma5", "vol_ma20"
    ]

    # âœ… train/val ë¶„í•  (DataFrame ê¸°ì¤€)
    cut = int(len(full_df) * 0.8)
    train_df, val_df = full_df.iloc[:cut], full_df.iloc[cut:]

    # âœ… ì •ê·œí™”
    train_scaled, val_scaled, scaler = normalize_features(train_df, val_df, feature_cols)

    # âœ… ì‹œí€€ìŠ¤ ìƒì„±
    window = 120
    train_seq = preprocess_sequence_input(pd.DataFrame(train_scaled, columns=feature_cols), window, feature_cols)
    val_seq = preprocess_sequence_input(pd.DataFrame(val_scaled, columns=feature_cols), window, feature_cols)

    # âœ… ë¼ë²¨ ìƒì„± (ì›ë³¸ df ê¸°ì¤€)
    train_labels = create_labels(train_df, window)[:-1]
    val_labels = create_labels(val_df, window)[:-1]
    # âœ… ê¸¸ì´ ë§ì¶”ê¸° (ì‹œí€€ìŠ¤ë¥¼ ë¼ë²¨ ìˆ˜ì— ë§ì¶° ìë¦„)
    train_seq = train_seq[:len(train_labels)]
    val_seq = val_seq[:len(val_labels)]
    # âœ… TensorDataset êµ¬ì„±
    train_set = torch.utils.data.TensorDataset(
        torch.tensor(train_seq, dtype=torch.float32),
        torch.tensor(train_labels, dtype=torch.long)
    )
    val_set = torch.utils.data.TensorDataset(
        torch.tensor(val_seq, dtype=torch.float32),
        torch.tensor(val_labels, dtype=torch.long)
    )

    # âœ… Dataloader
    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)

    # âœ… ëª¨ë¸ ì´ˆê¸°í™”
    input_dim = train_seq.shape[-1]
    model = AttentionGRUClassifier(input_dim=input_dim)

    # âœ… í•™ìŠµ ì‹¤í–‰
    train(model, dataloader=train_loader, val_loader=val_loader, epochs=500, log_freq=10, show_plot=True)

    